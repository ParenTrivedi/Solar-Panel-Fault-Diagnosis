Step 1: Import All Required Libraries

# Data Handling
import pandas as pd
import numpy as np

# Machine Learning Models & Preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Visualization
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns


Step 2: Load and Merge LPP & MPP Datasets
Hereâ€™s the Python code to:
âœ… Load LPP & MPP datasets (assumed in CSV format)
âœ… Merge them into a single dataframe
âœ… Add a column "Power_Condition" to distinguish LPP & MPP
âœ… Add a column "Fault_Type" to classify Normal, F1, and F5

import pandas as pd

# Load datasets for each fault type and power condition
f0_lpp = pd.read_csv("F0_LPP.csv")  # Normal Condition (Low Power Point)
f0_mpp = pd.read_csv("F0_MPP.csv")  # Normal Condition (Max Power Point)

f1_lpp = pd.read_csv("F1_LPP.csv")  # Inverter Fault (Low Power Point)
f1_mpp = pd.read_csv("F1_MPP.csv")  # Inverter Fault (Max Power Point)

f5_lpp = pd.read_csv("F5_LPP.csv")  # PV Array Mismatch (Low Power Point)
f5_mpp = pd.read_csv("F5_MPP.csv")  # PV Array Mismatch (Max Power Point)

# Add Fault Type and Power Condition Labels
f0_lpp["Fault_Type"] = "F0"
f0_mpp["Fault_Type"] = "F0"

f1_lpp["Fault_Type"] = "F1"
f1_mpp["Fault_Type"] = "F1"

f5_lpp["Fault_Type"] = "F5"
f5_mpp["Fault_Type"] = "F5"

f0_lpp["Power_Condition"] = "LPP"
f0_mpp["Power_Condition"] = "MPP"

f1_lpp["Power_Condition"] = "LPP"
f1_mpp["Power_Condition"] = "MPP"

f5_lpp["Power_Condition"] = "LPP"
f5_mpp["Power_Condition"] = "MPP"

# Merge all datasets
data = pd.concat([f0_lpp, f0_mpp, f1_lpp, f1_mpp, f5_lpp, f5_mpp], ignore_index=True)

# Display dataset info
print("Dataset Shape:", data.shape)
print("Unique Fault Types:", data["Fault_Type"].unique())
print("Unique Power Conditions:", data["Power_Condition"].unique())

# Display first few rows
print(data.head())

****************************************************************


Approach 1: Sampling 1000 Random Data Points Per Fault Type & Power Condition
If we just need a random subset of 1000 rows per condition, we can do:

python
Copy
Edit
import pandas as pd

# Function to sample 1000 rows per fault type & power condition
def sample_data(df, n=1000):
    return df.sample(n=min(n, len(df)), random_state=42)  # Ensure reproducibility

# Sample 1000 rows from each dataset
f0_lpp_sample = sample_data(f0_lpp)
f0_mpp_sample = sample_data(f0_mpp)
f1_lpp_sample = sample_data(f1_lpp)
f1_mpp_sample = sample_data(f1_mpp)
f5_lpp_sample = sample_data(f5_lpp)
f5_mpp_sample = sample_data(f5_mpp)

# Merge all sampled datasets
data = pd.concat([f0_lpp_sample, f0_mpp_sample, f1_lpp_sample, f1_mpp_sample, f5_lpp_sample, f5_mpp_sample], ignore_index=True)

# Display dataset shape
print("Dataset Shape:", data.shape)
print("Unique Fault Types:", data['Fault_Type'].unique())
print("Unique Power Conditions:", data['Power_Condition'].unique())
Approach 2: Selecting Data at Specific Time Intervals (Every Millisecond, Second, or Minute)
If the time column exists, we can group by time and take 1000 samples per unit time:

python
Copy
Edit
# Convert Time to appropriate format (if not already numeric)
data["Time"] = pd.to_numeric(data["Time"], errors='coerce')

# Group by rounded time (millisecond, second, or minute) & take first 1000 rows
data_filtered = data.groupby(data["Time"].round()).head(1000)

# Display dataset shape
print("Filtered Dataset Shape:", data_filtered.shape)
Which Approach to Use?
âœ… If time-based selection is required â†’ Use Approach 2
âœ… If random 1000 samples per condition are fine â†’ Use Approach 1

Do you want time-based filtering, or should we keep random sampling? ðŸš€


*************************************************************************************

Step 3: Data Preprocessing & Feature Selection
Now, weâ€™ll:
âœ… Handle missing values (if any)
âœ… Select relevant features (drop unnecessary columns)
âœ… Encode categorical labels (Fault_Type and Power_Condition)
âœ… Normalize data for machine learning


from sklearn.preprocessing import LabelEncoder, StandardScaler

# Drop unnecessary columns (if any, like Time if not needed)
features_to_drop = ["Time"] if "Time" in data.columns else []
data = data.drop(columns=features_to_drop)

# Handle missing values (if any)
data = data.dropna()

# Encode categorical labels
label_encoder = LabelEncoder()
data["Fault_Type"] = label_encoder.fit_transform(data["Fault_Type"])  # Convert F0, F1, F5 to numbers
data["Power_Condition"] = label_encoder.fit_transform(data["Power_Condition"])  # Convert LPP, MPP to numbers

# Define feature set and target variable
X = data.drop(columns=["Fault_Type"])  # Features (excluding Fault_Type)
y = data["Fault_Type"]  # Target variable

# Normalize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Display dataset shape after preprocessing
print("Processed Dataset Shape:", X.shape)
print("Feature Names:", X.columns)



Step 4: Train Random Forest & SVM Models
Now, weâ€™ll:
âœ… Split the dataset into training and testing sets
âœ… Train Random Forest & SVM classifiers
âœ… Evaluate model accuracy



from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Split dataset into training (80%) and testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Train Support Vector Machine (SVM) Classifier
svm_model = SVC(kernel="rbf", random_state=42)
svm_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)
y_pred_svm = svm_model.predict(X_test)

# Evaluate Random Forest
print("\nðŸ”¹ Random Forest Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))

# Evaluate SVM
print("\nðŸ”¹ SVM Model Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred_svm))
print(classification_report(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))



Step 5: 3D Visualization of Results
Now, weâ€™ll:
âœ… Plot a 3D scatter plot of features to visualize the classification
âœ… Use Matplotlib & Plotly for interactive visualization


import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

# Select three features for 3D plot (modify as needed)
feature_x = "Vpv"   # PV array voltage
feature_y = "Ipv"   # PV array current
feature_z = "Vdc"   # DC voltage

fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111, projection="3d")

# Scatter plot of actual fault types
scatter = ax.scatter(data[feature_x], data[feature_y], data[feature_z], 
                      c=data["Fault_Type"], cmap="coolwarm", edgecolors="k")

ax.set_xlabel(feature_x)
ax.set_ylabel(feature_y)
ax.set_zlabel(feature_z)
ax.set_title("3D Visualization of Fault Types")

# Add legend
legend = ax.legend(*scatter.legend_elements(), title="Fault Type")
ax.add_artist(legend)

plt.show()


for interactive 3d plot
Python Code: Interactive 3D Plot using Plotly

import plotly.express as px

fig = px.scatter_3d(data, x=feature_x, y=feature_y, z=feature_z, 
                     color=data["Fault_Type"].astype(str),
                     title="Interactive 3D Visualization of Fault Types",
                     labels={"color": "Fault Type"})
fig.show()


